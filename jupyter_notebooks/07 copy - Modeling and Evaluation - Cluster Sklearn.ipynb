{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0aStgWSO0E0E"
      },
      "source": [
        "# Cluster"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eLEkw5O0ECa"
      },
      "source": [
        "## Objectives\n",
        "\n",
        "* Fit and evaluate a cluster model to group similar customer behaviour\n",
        "* Understand profile for each cluster\n",
        "\n",
        "\n",
        "## Inputs\n",
        "\n",
        "* outputs/datasets/collection/TelcoCustomerChurn.csv\n",
        "* instructions on which variables to use for data cleaning and feature engineering. They are found on its respectives notebooks.\n",
        "\n",
        "## Outputs\n",
        "\n",
        "* Cluster Pipeline\n",
        "* Train Set\n",
        "* Feature importance plot\n",
        "* Clusters Description\n",
        "* Cluster Silhouette\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uWZXH9LwoQg"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Change working directory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We need to change the working directory from its current folder to its parent folder\n",
        "* We access the current directory with os.getcwd()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "current_dir = os.getcwd()\n",
        "current_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We want to make the parent of the parent of current directory the new current directory\n",
        "* os.path.dirname() gets the parent directory\n",
        "* os.chir() defines the new current directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "os.chdir(os.path.dirname(current_dir))\n",
        "print(\"You set a new current directory\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Confirm the new current directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "current_dir = os.getcwd()\n",
        "current_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pXKlJFX0iuM5"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mavJ8DibrcQ"
      },
      "source": [
        "# Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xk7DU_ekbtX8"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "df = (pd.read_csv(\"outputs/datasets/collection/TelcoCustomerChurn.csv\")\n",
        "      .drop(['customerID', 'TotalCharges', 'Churn', 'tenure' ],axis=1) \n",
        ")\n",
        "print(df.shape)\n",
        "df.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "krjAk78Tbyhv"
      },
      "source": [
        "# Cluster Pipeline considering all data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NZWZHhpYaDjf"
      },
      "source": [
        "## ML pipeline for Data Cleaning and Feature Engineering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C6keis6ao8LA"
      },
      "outputs": [],
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "### Feature Engineering\n",
        "from feature_engine.encoding import OrdinalEncoder\n",
        "from feature_engine.selection import SmartCorrelatedSelection\n",
        "\n",
        "def PipelineDataCleaningAndFeatureEngineering():\n",
        "  pipeline_base = Pipeline([\n",
        "       \n",
        "      (\"OrdinalCategoricalEncoder\",OrdinalEncoder(encoding_method='arbitrary',\n",
        "                                                  variables = [ 'gender', 'Partner', 'Dependents', 'PhoneService',\n",
        "                                                               'MultipleLines', 'InternetService', 'OnlineSecurity',\n",
        "                                                               'OnlineBackup','DeviceProtection', 'TechSupport', \n",
        "                                                               'StreamingTV', 'StreamingMovies','Contract', \n",
        "                                                               'PaperlessBilling', 'PaymentMethod'])\n",
        "      ),\n",
        "\n",
        "      (\"SmartCorrelatedSelection\",SmartCorrelatedSelection(variables=None, method=\"spearman\", \n",
        "                                                          threshold=0.6, selection_method=\"variance\") ),\n",
        "\n",
        "\n",
        "      (\"scaler\", StandardScaler()  ),\n",
        "      \n",
        "    \n",
        "  ])\n",
        "\n",
        "  return pipeline_base"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SrxSzwiJGzFP"
      },
      "source": [
        "## ML Pipeline for Cluster"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5hNJCrTiClFR"
      },
      "source": [
        "where `n_components` of PCA and `n_clusters` of KMeans will be updated"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eU953c8ZGzUQ"
      },
      "outputs": [],
      "source": [
        "### PCA\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "### Feat Scaling\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "### ML algorithms \n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "def PipelineCluster():\n",
        "  pipe = PipelineDataCleaningAndFeatureEngineering()\n",
        "  # pipe.steps.append([\"scaler\",StandardScaler()])   ##################\n",
        "  pipe.steps.append([\"PCA\",PCA(n_components=6, random_state=0)])\n",
        "  pipe.steps.append([\"model\",KMeans(n_clusters=4, random_state=0)])\n",
        "  return pipe\n",
        "\n",
        "PipelineCluster()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vzF21hNlG3H9"
      },
      "source": [
        "## ML Pipeline for a classifier to explain the clusters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g5m15PdrBzFE"
      },
      "source": [
        "We are considering a model that typically offers good results and features importance can be assessed with `.features_importance_`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "55qpqXLaG3d9"
      },
      "outputs": [],
      "source": [
        "# ### Feat Selection\n",
        "# from sklearn.feature_selection import SelectFromModel\n",
        "\n",
        "# from sklearn.ensemble import GradientBoostingClassifier\n",
        "# def PipelineClf2ExplainClusters():\n",
        "#    pipe = PipelineDataCleaningAndFeatureEngineering()\n",
        "#    # pipe.steps.append([\"scaler\",StandardScaler()])\n",
        "#    pipe.steps.append([\"feat_selection\",SelectFromModel(GradientBoostingClassifier(random_state=0))])\n",
        "#    pipe.steps.append([\"model\",GradientBoostingClassifier(random_state=0)])\n",
        "#    return pipe\n",
        "  \n",
        "# PipelineClf2ExplainClusters()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mrr31sD9DyvY"
      },
      "source": [
        "## Principal Component Analysis (PCA)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mC9WunDaCuQo"
      },
      "source": [
        "Apply PCA separately to find the most suitable `n_components`, update the value on ML Pipeline for Cluster"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "es49S65qqvRw"
      },
      "outputs": [],
      "source": [
        "pipeline_pca = PipelineDataCleaningAndFeatureEngineering()\n",
        "df_pca = pipeline_pca.fit_transform(df)\n",
        "print(df_pca.shape)\n",
        "# df_pca.hea"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WlABEj9Iw6Jr"
      },
      "source": [
        "Apply PCA component"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cM_Xsqxsrt5M"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "n_components = 6\n",
        "\n",
        "pca = PCA(n_components=n_components).fit(df_pca)\n",
        "x_PCA = pca.transform(df_pca) # array with transformed PCA\n",
        "\n",
        "ComponentsList = [\"Component \" + str(number) for number in range(n_components)]\n",
        "dfExplVarRatio = pd.DataFrame(\n",
        "    data= np.round(100 * pca.explained_variance_ratio_ ,2),\n",
        "    index=ComponentsList,\n",
        "    columns=['Explained Variance Ratio (%)'])\n",
        "\n",
        "PercentageOfDataExplained = dfExplVarRatio['Explained Variance Ratio (%)'].sum()\n",
        "\n",
        "print(f\"* The {n_components} components explain {round(PercentageOfDataExplained,2)}% of the data \\n\")\n",
        "print(dfExplVarRatio)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uw9NtDj4EtEJ"
      },
      "source": [
        "## Elbow Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "txwWrETeDFS2"
      },
      "source": [
        "Find the most suitable `n_clusters`, update the value on ML Pipeline for Cluster\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DOXb_H6iGx6U"
      },
      "source": [
        "Prepare data for analysis\n",
        "  * You need to clean and feature engineer your data using the pipeline without the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JVaMnb9vGyBw"
      },
      "outputs": [],
      "source": [
        "pipeline_cluster = PipelineCluster()\n",
        "pipeline_elbow = Pipeline(pipeline_cluster.steps[:-1])\n",
        "df_elbow = pipeline_elbow.fit_transform(df)\n",
        "\n",
        "print(df_elbow.shape,'\\n')\n",
        "df_elbow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q_1KLQujEvgi"
      },
      "source": [
        "Elbow Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TZBcHjt7EwFT"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set_style(\"whitegrid\")\n",
        "from yellowbrick.cluster import KElbowVisualizer\n",
        "\n",
        "visualizer = KElbowVisualizer(KMeans(random_state=0), k=(1,11))\n",
        "visualizer.fit(df_elbow) \n",
        "visualizer.show() \n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YQBjAlRsHhU4"
      },
      "source": [
        "## Fit Cluster Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kpxaylKk-6CQ"
      },
      "source": [
        "Quick recap in our data for training cluster pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zfKHc63v-6Zm"
      },
      "outputs": [],
      "source": [
        "X = df.copy()\n",
        "print(X.shape)\n",
        "X.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NfRpKC4Ykreg"
      },
      "source": [
        "Fit Cluster pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MAiyUpTWHjQh"
      },
      "outputs": [],
      "source": [
        "pipeline_cluster = PipelineCluster()\n",
        "pipeline_cluster.fit(X)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0L0iMkjJHXSI"
      },
      "source": [
        "## Add cluster labels to dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZKT5IjmTmei8"
      },
      "source": [
        "We add a column \"`Clusters`\" (with the cluster pipeline predictions) to the dataset\n",
        "* There are clusters with more user than other clusters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ow8B0xVdmlgK"
      },
      "outputs": [],
      "source": [
        "X['Clusters'] = pipeline_cluster['model'].labels_\n",
        "\n",
        "print(f\"* Clusters frequencies \\n{ X['Clusters'].value_counts(normalize=True).to_frame().round(2)} \\n\\n\")\n",
        "X['Clusters'].value_counts().sort_values().plot(kind='bar')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MVeCKYbRW9Xi"
      },
      "source": [
        "This is how our data look like from now\n",
        "  * Check the last column: `Clusters`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eAVrYJEqxYyG"
      },
      "outputs": [],
      "source": [
        "print(X.shape)\n",
        "X.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MnjHhYjXng2r"
      },
      "source": [
        "Here we are saving the cluster predictions for this pipeline to use in a fututre moment. We will get back to that soon"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FWgb0kPOWtMa"
      },
      "outputs": [],
      "source": [
        "cluster_predictions_with_all_variables = X['Clusters']\n",
        "cluster_predictions_with_all_variables"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hXtmFP_Ulpnd"
      },
      "source": [
        "## Evaluate Clusters silhouette"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dAJj6IZqmrxx"
      },
      "source": [
        "To evaluate clusters silhouete we need:\n",
        "  * data transformed (transform data in the pipeline wihout model step)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A4j4h-2Shlq4"
      },
      "outputs": [],
      "source": [
        "pipeline_silhouette = Pipeline(pipeline_cluster.steps[:-1])\n",
        "df_transformed = pipeline_silhouette.transform(df)\n",
        "df_transformed.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Plot silhouttes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from yellowbrick.cluster import SilhouetteVisualizer\n",
        "visualizer = SilhouetteVisualizer(Pipeline(pipeline_cluster.steps[-1:])[0] , colors='yellowbrick')\n",
        "\n",
        "visualizer.fit(df_transformed)\n",
        "visualizer.show()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pipeline_cluster"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sTWTf1rgkQ7b"
      },
      "source": [
        "## Fit a classifier, where target is cluster predictions and features remaining variables"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ipsJSzhhkUiu"
      },
      "source": [
        "We are in a moment where we have predictions from the cluster pipeline, but we don't have a meaning for them yet. \n",
        "* We seek to understand clusters' profile\n",
        "\n",
        "We need to find the most relevant variables, to define each cluster in terms of each relevant variable\n",
        "* Our new dataset has `Clusters`, which will be the **target for a classifier**. The most relevant features for this classifier, will be the most relevant variables when we use a classifier where the target is `Clusters`!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hP6sGUn0XyDm"
      },
      "source": [
        "We copy `X` to a DataFrame `df_clf`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OeLq81sm2yAg"
      },
      "outputs": [],
      "source": [
        "df_clf = X.copy()\n",
        "print(df_clf.shape)\n",
        "df_clf.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4b3Ei6Os5X3s"
      },
      "source": [
        "Split Train and Test sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cgHXehCVyzUl"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test,y_train, y_test = train_test_split(\n",
        "                                    df_clf.drop(['Clusters'],axis=1),\n",
        "                                    df_clf['Clusters'],\n",
        "                                    test_size=0.2,\n",
        "                                    random_state=0\n",
        "                                    )\n",
        "\n",
        "print(X_train.shape, X_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6EZUk-uV5aN8"
      },
      "source": [
        "Create and fit a classifier pipeline to learn most relevant features that determine a cluster "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g5m15PdrBzFE"
      },
      "source": [
        "We are considering a model that typically offers good results and features importance can be assessed with `.features_importance_`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### Feat Selection\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "\n",
        "### ML algorithm\n",
        "# from sklearn.ensemble import ExtraTreesClassifier\n",
        "# from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "\n",
        "# from sklearn.tree import DecisionTreeClassifier\n",
        "# from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier \n",
        "# from sklearn.ensemble import ExtraTreesClassifier\n",
        "# from sklearn.ensemble import AdaBoostClassifier\n",
        "# from xgboost import XGBClassifier\n",
        "\n",
        "def PipelineClf2ExplainClusters():\n",
        "   pipe = PipelineDataCleaningAndFeatureEngineering()\n",
        "   # pipe.steps.append([\"scaler\",StandardScaler()])\n",
        "   pipe.steps.append([\"feat_selection\",SelectFromModel(GradientBoostingClassifier(random_state=0))])\n",
        "   pipe.steps.append([\"model\",GradientBoostingClassifier(random_state=0)])\n",
        "   return pipe\n",
        "  \n",
        "PipelineClf2ExplainClusters()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3R7xdg1Av0Ce"
      },
      "outputs": [],
      "source": [
        "pipeline_clf_cluster = PipelineClf2ExplainClusters()\n",
        "pipeline_clf_cluster.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z05LMFoZ4T2K"
      },
      "source": [
        "## Evaluate classifier performance on Train and Test Sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M1iqL2Kc544K"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(y_train, pipeline_clf_cluster.predict(X_train)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Oo4xJMZ615p"
      },
      "outputs": [],
      "source": [
        "print(classification_report(y_test, pipeline_clf_cluster.predict(X_test)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MEwjHBSh5ejG"
      },
      "source": [
        "## Assess Most Important Features that define a cluster"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# pipeline_cluster['SmartCorrelatedSelection'].correlated_feature_sets_ # 3 groups"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# pipeline_cluster['SmartCorrelatedSelection'].features_to_drop_ \n",
        "# there was 17, 5 are dropped, we keep 12"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Pipeline(pipeline_cluster.steps[:data_cleaning_feat_eng_steps])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pipeline_clf_cluster['SmartCorrelatedSelection'].correlated_feature_sets_ #['feat_selection'].get_support()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pipeline_clf_cluster['SmartCorrelatedSelection'].features_to_drop_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BG5ztHxsKcd5"
      },
      "outputs": [],
      "source": [
        "# after data cleaning and feat engineering, the feature space changes\n",
        "data_cleaning_feat_eng_steps = 2 # how many data cleaning and feature engineering does your pipeline have?\n",
        "columns_after_data_cleaning_feat_eng = (Pipeline(pipeline_clf_cluster.steps[:data_cleaning_feat_eng_steps])\n",
        "                                        .transform(X_train)\n",
        "                                        .columns)\n",
        "\n",
        "best_features = columns_after_data_cleaning_feat_eng[pipeline_clf_cluster['feat_selection'].get_support()].to_list()\n",
        "\n",
        "# create DataFrame to display feature importance\n",
        "df_feature_importance = (pd.DataFrame(data={\n",
        "          'Feature': columns_after_data_cleaning_feat_eng[pipeline_clf_cluster['feat_selection'].get_support()],\n",
        "          'Importance': pipeline_clf_cluster['model'].feature_importances_})\n",
        "  .sort_values(by='Importance', ascending=False)\n",
        "  )\n",
        "\n",
        "best_features = df_feature_importance['Feature'].to_list() # reassign best features in importance order\n",
        "\n",
        "# Most important features statement and plot\n",
        "print(f\"* These are the {len(best_features)} most important features in descending order. \"\n",
        "      f\"The model was trained on them: \\n{best_features} \\n\")\n",
        "df_feature_importance.plot(kind='bar',x='Feature',y='Importance')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qgul0EF9nx_E"
      },
      "source": [
        "We will store the best_features for future usage. We will get back to that soon"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YzyMkwHznyG8"
      },
      "outputs": [],
      "source": [
        "best_features_pipeline_all_variables = best_features\n",
        "best_features_pipeline_all_variables"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J2ywCxJmkRQn"
      },
      "source": [
        "## Cluster Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hZMr-wiudEkb"
      },
      "source": [
        "* Table with description for all Clusters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_lpRVDqTdEul"
      },
      "outputs": [],
      "source": [
        "def Clusters_IndividualDescription(EDA_Cluster,cluster):\n",
        "\n",
        "  ClustersDescription = pd.DataFrame(columns=EDA_Cluster.columns)\n",
        "  for col in EDA_Cluster.columns:\n",
        "    \n",
        "    try:  # eventually a given cluster will have only mssing data for a given variable\n",
        "      \n",
        "      if EDA_Cluster[col].dtypes == 'object':\n",
        "        \n",
        "        top_frequencies = EDA_Cluster.dropna(subset=[col])[[col]].value_counts(normalize=True).nlargest(n=3)\n",
        "        Description = ''\n",
        "        \n",
        "        for x in range(len(top_frequencies)):\n",
        "          freq = top_frequencies.iloc[x]\n",
        "          category = top_frequencies.index[x][0]\n",
        "          CategoryPercentage = int(round(freq*100,0))\n",
        "          statement =  f\"'{category}': {CategoryPercentage}% , \"  \n",
        "          Description = Description + statement\n",
        "        \n",
        "        ClustersDescription.at[0,col] = Description[:-2]\n",
        "\n",
        "\n",
        "      \n",
        "      elif EDA_Cluster[col].dtypes in ['float', 'int']:\n",
        "        DescStats = EDA_Cluster.dropna(subset=[col])[[col]].describe()\n",
        "        Q1 = int(round(DescStats.iloc[4,0],0))\n",
        "        Q3 = int(round(DescStats.iloc[6,0],0))\n",
        "        Description = f\"{Q1} -- {Q3}\"\n",
        "        ClustersDescription.at[0,col] = Description\n",
        "    \n",
        "    \n",
        "    except Exception as e:\n",
        "      ClustersDescription.at[0,col] = 'Not available'\n",
        "      print(f\"** Error Exception: {e} - cluster {cluster}, variable {col}\")\n",
        "  \n",
        "  ClustersDescription['Cluster'] = str(cluster)\n",
        "  \n",
        "  return ClustersDescription\n",
        "\n",
        "\n",
        "def DescriptionAllClusters(df_cluster_profile):\n",
        "\n",
        "  DescriptionAllClusters = pd.DataFrame(columns=df_cluster_profile.drop(['Clusters'],axis=1).columns)\n",
        "  for cluster in df_cluster_profile.sort_values(by='Clusters')['Clusters'].unique():\n",
        "    \n",
        "      EDA_ClusterSubset = df_cluster_profile.query(f\"Clusters == {cluster}\").drop(['Clusters'],axis=1)\n",
        "      ClusterDescription = Clusters_IndividualDescription(EDA_ClusterSubset,cluster)\n",
        "      DescriptionAllClusters = DescriptionAllClusters.append(ClusterDescription)\n",
        "\n",
        "  \n",
        "  DescriptionAllClusters.set_index(['Cluster'],inplace=True)\n",
        "  return DescriptionAllClusters\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OHo7wmH68AYc"
      },
      "source": [
        "* Cluster distribution per Variable (absolute and relative levels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NN23X2dT8AeA"
      },
      "outputs": [],
      "source": [
        "import plotly.express as px\n",
        "def cluster_distribution_per_variable(df,target):\n",
        "\n",
        "\n",
        "  df_bar_plot = df.value_counts([\"Clusters\", target]).reset_index() \n",
        "  df_bar_plot.columns = ['Clusters',target,'Count']\n",
        "  df_bar_plot[target] = df_bar_plot[target].astype('object')\n",
        "\n",
        "  print(f\"Clusters distribution across {target} levels\")\n",
        "  fig = px.bar(df_bar_plot, x='Clusters',y='Count',color=target,width=800, height=500)\n",
        "  fig.update_layout(xaxis=dict(tickmode= 'array',tickvals= df['Clusters'].unique()))\n",
        "  fig.show()\n",
        "\n",
        "\n",
        "  df_relative = (df\n",
        "                 .groupby([\"Clusters\", target])\n",
        "                 .size()\n",
        "                 .groupby(level=0)\n",
        "                 .apply(lambda x:  100*x / x.sum())\n",
        "                 .reset_index()\n",
        "                 .sort_values(by=['Clusters'])\n",
        "                 )\n",
        "  df_relative.columns = ['Clusters',target,'Relative Percentage (%)']\n",
        " \n",
        "\n",
        "  print(f\"Relative Percentage (%) of {target} in each cluster\")\n",
        "  fig = px.line(df_relative, x='Clusters',y='Relative Percentage (%)',color=target,width=800, height=500)\n",
        "  fig.update_layout(xaxis=dict(tickmode= 'array',tickvals= df['Clusters'].unique()))\n",
        "  fig.update_traces(mode='markers+lines')\n",
        "  fig.show()\n",
        " \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ou5ymnDkJJTl"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73J7J65v4O_d"
      },
      "source": [
        "We will study the profile for the main variables that define a cluster\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PztdhjGl4Vkg"
      },
      "outputs": [],
      "source": [
        "df_cluster_profile = df_clf.copy()\n",
        "df_cluster_profile = df_cluster_profile.filter(items=best_features + ['Clusters'], axis=1)\n",
        "df_cluster_profile.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mfJRrFc7wzu"
      },
      "source": [
        "Load Churn levels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fSRSNqiF4mnm"
      },
      "outputs": [],
      "source": [
        "df_churn = pd.read_csv(\"outputs/datasets/collection/TelcoCustomerChurn.csv\").filter(['Churn'])\n",
        "df_churn['Churn'] = df_churn['Churn'].astype('object')\n",
        "df_churn.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KtD0Y3NdJOhm"
      },
      "source": [
        "### Cluster profile on most important features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T0dCJG1zG13D"
      },
      "source": [
        "Considering `df_cluster_profile` and `df_churn`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LDhycaSEdORm"
      },
      "outputs": [],
      "source": [
        "pd.set_option('display.max_colwidth', None)\n",
        "clusters_profile = DescriptionAllClusters(pd.concat([df_cluster_profile,df_churn], axis=1))\n",
        "clusters_profile"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2SS6CCCb74lH"
      },
      "source": [
        "### Clusters distribution across Churn levels & Relative Percentage of Churn in each cluster"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kwEUdPI2NHOb"
      },
      "outputs": [],
      "source": [
        "df_cluster_vs_churn=  df_churn.copy()\n",
        "df_cluster_vs_churn['Clusters'] = X['Clusters']\n",
        "cluster_distribution_per_variable(df=df_cluster_vs_churn, target='Churn')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iEfOvjx8ZhAH"
      },
      "source": [
        "# Fit New Cluster Pipeline only on most important features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MFno4XnSZlZV"
      },
      "source": [
        "In order to reduce feature space, we will study the trade-off between the previous Cluster Pipeline (fitted with all variables) and Pipeline with the variables that are most important to define the clusters from the previous pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ROH8cre2PHWx"
      },
      "outputs": [],
      "source": [
        "best_features_pipeline_all_variables"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cLOL2zr4Jr68"
      },
      "source": [
        "## Define trade-off and metrics to compare new and previous Cluster Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJyxkSowZ9Cm"
      },
      "source": [
        "To evaluate this trade-off we will\n",
        "1. Conduct a PCA analysis, with the same amount of components from \n",
        "previous study, in a dataset only with `best_features_pipeline_all_variables` and check if new components have still almost all data variance \n",
        "2. Conduct a elbow study and check if the same number of clusters is suggested\n",
        "3. Fit new cluster pipeline and compare if the both clusters predictions are \"equivalent\"\n",
        "4. Compare silhoutte score\n",
        "5. Fit a classifier to explain cluster, and check if performance on Train and Test sets is similar\n",
        "6. Check if the most important features for the classifier are the same.\n",
        "7. Compare if the cluster profile from both cases are \"equivalent\"\n",
        "\n",
        "If we are happy to say **yes** for them, you can use a cluster pipeline with a reduced feature space!\n",
        "* The **gain** is that in real time (which is the major purpose of Machine Learning) you will need less variables for predicting cluster for your prospects."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DxyTpm4EJx4s"
      },
      "source": [
        "## Consider the data with the most relevant variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lQu9033oZ6r9"
      },
      "outputs": [],
      "source": [
        "df_reduced = df.filter(best_features_pipeline_all_variables)\n",
        "df_reduced.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ub9_YoIeaS5"
      },
      "source": [
        "## Rewrite ML pipeline for Data Cleaning and Feature Engineering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PrlQuieZeaS6"
      },
      "outputs": [],
      "source": [
        "def PipelineDataCleaningAndFeatureEngineering():\n",
        "  pipeline_base = Pipeline([\n",
        "        # we updated the pipeline, considering only the most important variables from previous pipeline      \n",
        "       (\"OrdinalCategoricalEncoder\",OrdinalEncoder(encoding_method='arbitrary',\n",
        "                                                  # variables = ['PaymentMethod', 'InternetService',\n",
        "                                                  #              'DeviceProtection','OnlineSecurity']\n",
        "                                                  variables = ['OnlineBackup', 'PhoneService',\n",
        "                                                                'Partner',\t'Dependents'] ) ),\n",
        "\n",
        "        # it doesnt need SmartCorrelation\n",
        "\n",
        "        (\"scaler\", StandardScaler()  ),\n",
        "\n",
        "    ])\n",
        "\n",
        "  return pipeline_base\n",
        "\n",
        "PipelineDataCleaningAndFeatureEngineering()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def PipelineCluster():\n",
        "  pipe = PipelineDataCleaningAndFeatureEngineering()\n",
        "  # pipe.steps.append([\"scaler\",StandardScaler()])   ##################\n",
        "  # pipe.steps.append([\"PCA\",PCA(n_components=6, random_state=0)])\n",
        "  pipe.steps.append([\"model\",KMeans(n_clusters=4, random_state=0)])\n",
        "  return pipe\n",
        "\n",
        "PipelineCluster()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S4JsUASTfCiE"
      },
      "source": [
        "## Apply PCA and compare to previous PCA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "YOU DONT NEED TO APPLY PCA, SINCE YOU HAVE ALREADY THE MOST IMPORTANT FEATURES!!!!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GJs6VtKvfCiS"
      },
      "source": [
        "It needs the dataset after data cleaning and feature engineering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sQ09ogO8fCiS"
      },
      "outputs": [],
      "source": [
        "# pipeline_pca = PipelineDataCleaningAndFeatureEngineering()\n",
        "# df_pca = pipeline_pca.fit_transform(df_reduced)\n",
        "# print(df_pca.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "grI6_LgYfCiU"
      },
      "source": [
        "Apply PCA component"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "upco-DU-fCiU"
      },
      "outputs": [],
      "source": [
        "# n_components = 5\n",
        "\n",
        "# pca = PCA(n_components=n_components).fit(df_pca)\n",
        "# x_PCA = pca.transform(df_pca) # array with transformed PCA\n",
        "\n",
        "# ComponentsList = [\"Component \" + str(number) for number in range(n_components)]\n",
        "# dfExplVarRatio = pd.DataFrame(\n",
        "#     data= np.round(100 * pca.explained_variance_ratio_ ,2),\n",
        "#     index=ComponentsList,\n",
        "#     columns=['Explained Variance Ratio (%)'])\n",
        "\n",
        "# PercentageOfDataExplained = dfExplVarRatio['Explained Variance Ratio (%)'].sum()\n",
        "\n",
        "# print(f\"* The {n_components} components explain {round(PercentageOfDataExplained,2)}% of the data \\n\")\n",
        "# print(dfExplVarRatio)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D57ncdQ7hBXe"
      },
      "source": [
        "## Apply Elbow analysis and compare to previous Elbow analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i_Iw99sEhBXr"
      },
      "source": [
        "Prepare data for analysis\n",
        "  * You need to clean and feature engineer your data using the pipeline without the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# PipelineCluster()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D0wqQOM3hBXr"
      },
      "outputs": [],
      "source": [
        "pipeline_cluster = PipelineCluster()\n",
        "pipeline_elbow = Pipeline(pipeline_cluster.steps[:-1])\n",
        "df_elbow = pipeline_elbow.fit_transform(df_reduced)\n",
        "\n",
        "print(df_elbow.shape,'\\n')\n",
        "df_elbow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r_1H05FKhBXs"
      },
      "source": [
        "Elbow Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nsAJW4s0hBXt"
      },
      "outputs": [],
      "source": [
        "from yellowbrick.cluster import KElbowVisualizer\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "visualizer = KElbowVisualizer(KMeans(random_state=0), k=(1,11))\n",
        "visualizer.fit(df_elbow) \n",
        "visualizer.show() \n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pFVtLmU5hWyn"
      },
      "source": [
        "The same number of clusters is suggested! :)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f_T1gtprhe8W"
      },
      "source": [
        "## Fit New Cluster Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZEtzurhOhe8W"
      },
      "source": [
        "We set X as our training set for cluster. It is a copy of df_reduced"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BIZRR3wChe8X"
      },
      "outputs": [],
      "source": [
        "X = df_reduced.copy()\n",
        "print(X.shape)\n",
        "X.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O-z3ST3nhe8Z"
      },
      "source": [
        "Fit Cluster pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AAaEPy3Uhe8Z"
      },
      "outputs": [],
      "source": [
        "pipeline_cluster = PipelineCluster()\n",
        "pipeline_cluster.fit(X)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Pdrt5bdKLDF"
      },
      "source": [
        "## Add cluster predictions to dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CdFwZk1ihe8b"
      },
      "source": [
        "We add a column \"Cluster\" to the data and check clusters distribution\n",
        "* Clusters don't look to be imbalanced"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wUwR8vCEhe8b"
      },
      "outputs": [],
      "source": [
        "X['Clusters'] = pipeline_cluster['model'].labels_\n",
        "\n",
        "print(f\"* Clusters frequencies \\n{ X['Clusters'].value_counts(normalize=True).to_frame().round(2)} \\n\\n\")\n",
        "X['Clusters'].value_counts().sort_values().plot(kind='bar');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QUxjeMypKOUe"
      },
      "source": [
        "## Compare current cluster labels to previous cluster labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w6721kuGiII6"
      },
      "source": [
        "We just fitted a new cluster pipeline and want to compare if its predictions are \"equivalent\" from the previous cluster"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GAS2dDXQhe8c"
      },
      "source": [
        "These are the predictions from the **previous** cluster pipeline "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pbVaUAABhe8c"
      },
      "outputs": [],
      "source": [
        "cluster_predictions_with_all_variables"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xUiHzLIwimaD"
      },
      "source": [
        "And these are the predictions from **current** cluster pipeline (trained with `df_reduced`)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_kC7cKKCiwD5"
      },
      "outputs": [],
      "source": [
        "cluster_anwers_with_major_variables = X['Clusters'] \n",
        "cluster_anwers_with_major_variables"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UgkF8MyyAjTF"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Jt-2n1GidBa"
      },
      "source": [
        "We use a confusion matrix to evaluate if the predictions of both pipelines are **\"equivalent\"**\n",
        "* We say equivalent in quotes, because we can't expect that a cluster label 0 in the previous cluster will have the same label in the current cluster. Eventually label 0 in previous cluster pipeline will be a different label in current cluster pipeline\n",
        "* When we reach this **equivalence**, it means both clusters \"clustered\" in a similar way but with different labels. And this is fine, since the label itself doesn't have meaning. We will look for the meaning after. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tLy37N1TiiSx"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "print(confusion_matrix(cluster_predictions_with_all_variables, cluster_anwers_with_major_variables))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aXrL1GUnjz1t"
      },
      "source": [
        "Conclusion\n",
        "* We see that both Clusters Pipelines are not predicting the data 100% equivalent, since few data points have different meaning for each pipeline. \n",
        "* However this is fine, and we say yes for this criteria. This is part of the trade-off we are up to."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VXSU4Tk3mIBv"
      },
      "source": [
        "## Evaluate current Clusters silhouette"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Fwxo2fPmIB8"
      },
      "source": [
        "* To evaluate clusters silhouete we need:\n",
        "  * data transformed (transform data in the pipeline wihout model step)\n",
        "  * clusters arrays"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1IupILQzmIB9"
      },
      "outputs": [],
      "source": [
        "pipeline_silhouette = Pipeline(pipeline_cluster.steps[:-1])\n",
        "df_transformed = pipeline_silhouette.transform(df_reduced)\n",
        "df_transformed.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ozvP79HimICA"
      },
      "source": [
        "Evaluate Cluster Silhouette"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ohVwnguomICA"
      },
      "outputs": [],
      "source": [
        "from yellowbrick.cluster import SilhouetteVisualizer\n",
        "visualizer = SilhouetteVisualizer(Pipeline(pipeline_cluster.steps[-1:])[0] , colors='yellowbrick')\n",
        "\n",
        "visualizer.fit(df_transformed)\n",
        "visualizer.show()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h98ivtaamYAx"
      },
      "source": [
        "The silhoutte score from both pipelines are similar! Now it has even increased a bit :)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fcPfalkwmomc"
      },
      "source": [
        "## Rewrite ML Pipeline for a classifier to explain the clusters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cIcOSR42momd"
      },
      "source": [
        "We want again to explain the major variables for our clusters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ZClC5E5mome"
      },
      "source": [
        "We copy `X` to a DataFrame `df_clf`, just to separate the concerns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_XoL6tuRmomf"
      },
      "outputs": [],
      "source": [
        "df_clf = X.copy()\n",
        "print(df_clf.shape)\n",
        "df_clf.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PSfwpL-Fmomf"
      },
      "source": [
        "Split Train and Test sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sPyXs27Kmomf"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test,y_train, y_test = train_test_split(\n",
        "                                    df_clf.drop(['Clusters'],axis=1),\n",
        "                                    df_clf['Clusters'],\n",
        "                                    test_size=0.2,\n",
        "                                    random_state=0\n",
        "                                    )\n",
        "\n",
        "print(X_train.shape, X_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0W-cV2ts8A_N"
      },
      "source": [
        "Rewrite pipeline to explain clusters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lm63GRYP8BIV"
      },
      "outputs": [],
      "source": [
        "def PipelineClf2ExplainClusters():\n",
        "   pipe = PipelineDataCleaningAndFeatureEngineering()\n",
        "   # no feature selection step\n",
        "   # pipe.steps.append([\"feat_selection\",SelectFromModel(GradientBoostingClassifier(random_state=0))])\n",
        "   pipe.steps.append([\"model\",GradientBoostingClassifier(random_state=0)])\n",
        "   return pipe\n",
        "\n",
        "PipelineClf2ExplainClusters()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lkaBhgjOmomg"
      },
      "source": [
        "## Fit a classifier, where target is cluster labels and features remaining variables"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zU6mwsFYmomg"
      },
      "source": [
        "Create and fit a classifier pipeline to learn the feature importance when defining a cluster"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w3liI7qjmomg"
      },
      "outputs": [],
      "source": [
        "pipeline_clf_cluster = PipelineClf2ExplainClusters()\n",
        "pipeline_clf_cluster.fit(X_train,y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6hCk6Swrmomh"
      },
      "source": [
        "## Evaluate classifier performance on Train and Test Sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZjXpF0x8momh"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(y_train, pipeline_clf_cluster.predict(X_train)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l-Obn_Hcmomh"
      },
      "outputs": [],
      "source": [
        "print(classification_report(y_test, pipeline_clf_cluster.predict(X_test)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X-KlCS6MnBLk"
      },
      "source": [
        "The performance on Train and Test sets are similar, comparing to the previous pipeline! :)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G07251XWmomh"
      },
      "source": [
        "## Assess Most Important Features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BE0E0QkbKv-Z"
      },
      "source": [
        "They help the most to define a cluster, compare with previous pipeline\n",
        "* we can use .feature_importances_ since it is a tree based model\n",
        "* since we don't have feature selection step, we just get best_features as the Xtrain columns\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Pipeline(pipeline_clf_cluster.steps[:data_cleaning_feat_eng_steps]).transform(X_train).columns\n",
        "pipeline_clf_cluster"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IMTUNBYN8fyf"
      },
      "outputs": [],
      "source": [
        "best_features = X_train.columns.to_list()\n",
        "\n",
        "\n",
        "# after data cleaning and feat engineering, the feature space changes\n",
        "# data_cleaning_feat_eng_steps = 1 # how many data cleaning and feature engineering does your pipeline have?\n",
        "# columns_after_data_cleaning_feat_eng = (Pipeline(pipeline_clf_cluster.steps[:data_cleaning_feat_eng_steps])\n",
        "#                                         .transform(X_train)\n",
        "#                                         .columns)\n",
        "\n",
        "# best_features = columns_after_data_cleaning_feat_eng[pipeline_clf_cluster['feat_selection'].get_support()].to_list()\n",
        "\n",
        "\n",
        "# create DataFrame to display feature importance\n",
        "df_feature_importance = (pd.DataFrame(data={\n",
        "    'Feature':best_features,\n",
        "    'Importance': pipeline_clf_cluster['model'].feature_importances_})\n",
        ".sort_values(by='Importance', ascending=False)\n",
        ")\n",
        "\n",
        "best_features = df_feature_importance['Feature'].to_list()\n",
        "\n",
        "# Most important features statement and plot\n",
        "print(f\"* These are the {len(best_features)} most important features in descending order. \"\n",
        "      f\"The model was trained on them: \\n{df_feature_importance['Feature'].to_list()}\")\n",
        "\n",
        "df_feature_importance.plot(kind='bar',x='Feature',y='Importance')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9q1TJSqdI6xK"
      },
      "source": [
        "## Cluster Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C8EMIqE5I6xP"
      },
      "source": [
        "We will study the profile for the main variables that define a cluster\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TEg92vdnI6xP"
      },
      "outputs": [],
      "source": [
        "df_cluster_profile = df_clf.copy()\n",
        "df_cluster_profile = df_cluster_profile.filter(items=best_features + ['Clusters'], axis=1)\n",
        "df_cluster_profile.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "789CeA0WI6xQ"
      },
      "source": [
        "Load Churn levels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nx335aqtI6xR"
      },
      "outputs": [],
      "source": [
        "df_churn = pd.read_csv(\"outputs/datasets/collection/TelcoCustomerChurn.csv\").filter(['Churn'])\n",
        "df_churn['Churn'] = df_churn['Churn'].astype('object')\n",
        "df_churn.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-7jYzhtI6xR"
      },
      "source": [
        "### Cluster profile on most important features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Yi0fwrTI6xR"
      },
      "source": [
        "Considering `df_cluster_profile` and `df_churn`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "urBmw5HJI6xS"
      },
      "outputs": [],
      "source": [
        "pd.set_option('display.max_colwidth', None)\n",
        "clusters_profile = DescriptionAllClusters(pd.concat([df_cluster_profile,df_churn], axis=1))\n",
        "clusters_profile"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LDJRuBBgI6xS"
      },
      "source": [
        "### Clusters distribution across Churn levels & Relative Percentage of Churn in each cluster"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZjMnAqYKI6xS"
      },
      "outputs": [],
      "source": [
        "df_cluster_vs_churn=  df_churn.copy()\n",
        "df_cluster_vs_churn['Clusters'] = X['Clusters']\n",
        "cluster_distribution_per_variable(df=df_cluster_vs_churn, target='Churn')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4xhPLbC4dwXL"
      },
      "source": [
        "## Which pipeline should I keep?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-8qASh5k1jph"
      },
      "source": [
        "Let's recap the criteria we consider to evaluate the **trade-off**\n",
        "1. Conduct a PCA analysis, with the same amount of components from \n",
        "previous study, in a dataset only with `best_features_pipeline_all_variables` and check if new components have still almost all data variance \n",
        "2. Conduct a elbow study and check if the same number of clusters is suggested\n",
        "3. Fit new cluster pipeline and compare if the both clusters predictions are \"equivalent\"\n",
        "4. Compare silhoutte score\n",
        "5. Fit a classifier to explain cluster, and check if performance on Train and Test sets is similar\n",
        "6. Check if the most important features for the classifier are the same.\n",
        "7. Compare if the cluster profile from both cases are \"equivalent\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hPbS7JYN101K"
      },
      "source": [
        "We are happy with all criteria above for the new Cluster Pipeline\n",
        "\n",
        "* All 7 criteria support the second pipeline. In addition, there is a great gain to have less variables for predicting live data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a4HsuSuqd0g_"
      },
      "outputs": [],
      "source": [
        "pipeline_cluster"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4zxpjktKd1n6"
      },
      "source": [
        "# Push files to Repo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5i9X1oOORAQc"
      },
      "source": [
        "\n",
        "We will generate the following files\n",
        "\n",
        "* Cluster Pipeline\n",
        "* Train Set\n",
        "* Feature importance plot\n",
        "* Clusters Description\n",
        "* Cluster Silhouette\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ySBIrV1Q4cY"
      },
      "outputs": [],
      "source": [
        "import joblib\n",
        "import os\n",
        "\n",
        "version = 'v1'\n",
        "file_path = f'outputs/ml_pipeline/cluster_analysis/{version}'\n",
        "\n",
        "try:\n",
        "  os.makedirs(name=file_path)\n",
        "except Exception as e:\n",
        "  print(e)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6y9-0fisd5cl"
      },
      "source": [
        "## Cluster pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xfv9k5xMd7fv"
      },
      "outputs": [],
      "source": [
        "pipeline_cluster"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IsphnIR84hJ4"
      },
      "outputs": [],
      "source": [
        "joblib.dump(value=pipeline_cluster ,\n",
        "            filename=f\"{file_path}/cluster_pipeline.pkl\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ORnkwG6d74O"
      },
      "source": [
        "## Train Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QqcwHaVwd9Ff"
      },
      "outputs": [],
      "source": [
        "print(df_reduced.shape)\n",
        "df_reduced.head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M26MiJ9Y485Q"
      },
      "outputs": [],
      "source": [
        "df_reduced.to_csv(f\"{file_path}/TrainSet.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eX_lcQVXaV0p"
      },
      "source": [
        "## Most important features plot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o9datNfsLCVV"
      },
      "source": [
        "These are the features that define a cluster"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FYeoH7fjaV8J"
      },
      "outputs": [],
      "source": [
        "df_feature_importance.plot(kind='bar',x='Feature',y='Importance')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sr0cVVQsaZqk"
      },
      "outputs": [],
      "source": [
        "df_feature_importance.plot(kind='bar',x='Feature',y='Importance')\n",
        "plt.savefig(f\"{file_path}/features_define_cluster.png\", bbox_inches='tight', dpi=150)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GX3Z5ivNd9mw"
      },
      "source": [
        "## Cluster Profile"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tw-mEnI8d_Bv"
      },
      "outputs": [],
      "source": [
        "clusters_profile"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7G5CsAl738p7"
      },
      "outputs": [],
      "source": [
        "clusters_profile.to_csv(f\"{file_path}/clusters_profile.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RObeac1HQq5a"
      },
      "source": [
        "## Cluster silhouette plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "visualizer = SilhouetteVisualizer(Pipeline(pipeline_cluster.steps[-1:])[0] , colors='yellowbrick')\n",
        "\n",
        "visualizer.fit(df_transformed)\n",
        "visualizer.show()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(figsize=(7,5))\n",
        "fig = SilhouetteVisualizer(Pipeline(pipeline_cluster.steps[-1:])[0] , colors='yellowbrick', ax=axes)\n",
        "fig.fit(df_transformed)\n",
        "\n",
        "plt.savefig(f\"{file_path}/clusters_silhouette.png\", bbox_inches='tight',dpi=150)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TeIEUrWkJ-6_"
      },
      "source": [
        "Good job, clear the cell ouputs, run git commands to add, commit and push files to the repo. Next, we will move on to create our dashboard!"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Modeling and Evaluation - Cluster Sklearn.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "interpreter": {
      "hash": "8b8334dab9339717f727a1deaf837b322d7a41c20d15cc86be99a8e69ceec8ce"
    },
    "kernelspec": {
      "display_name": "Python 3.8.12 64-bit ('3.8.12': pyenv)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    },
    "orig_nbformat": 2
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
